#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DB ê¸°ë°˜ ìºì‹œ ë§¤ë‹ˆì € v1.0

ëª©ì :
- ì¼ë³„ ì‹œì„¸ ë°ì´í„° ëˆ„ì  ì €ì¥
- ì„¹í„° í†µê³„ í”„ë¦¬ì»´í“¨íŒ…
- ë°±í…ŒìŠ¤íŠ¸ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì§€ì›

ì¥ì :
- API í˜¸ì¶œ 90% ì ˆê° (ì¦ë¶„ ì—…ë°ì´íŠ¸)
- ì˜êµ¬ ì €ì¥ + ì´ë ¥ ê´€ë¦¬
- ë¹ ë¥¸ ì¡°íšŒ (ì¸ë±ìŠ¤)
"""

import sqlite3
import logging
from pathlib import Path
from datetime import date, datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
import pandas as pd
import numpy as np
from contextlib import contextmanager

logger = logging.getLogger(__name__)

class DBCacheManager:
    """DB ê¸°ë°˜ ìºì‹œ ë§¤ë‹ˆì € (SQLite)"""
    
    def __init__(self, db_path: str = 'cache/stock_data.db'):
        """
        Args:
            db_path: DB íŒŒì¼ ê²½ë¡œ
        """
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        
        self._init_db()
        logger.info(f"âœ… DBCacheManager ì´ˆê¸°í™”: {self.db_path}")
    
    def _init_db(self):
        """DB ì´ˆê¸°í™” (ìŠ¤í‚¤ë§ˆ ì ìš©)"""
        schema_file = Path('db_schema.sql')
        
        if not schema_file.exists():
            logger.warning(f"âš ï¸ ìŠ¤í‚¤ë§ˆ íŒŒì¼ ì—†ìŒ: {schema_file}")
            return
        
        with open(schema_file, 'r', encoding='utf-8') as f:
            schema_sql = f.read()
        
        conn = sqlite3.connect(str(self.db_path))
        try:
            conn.executescript(schema_sql)
            conn.commit()
            logger.info("âœ… DB ìŠ¤í‚¤ë§ˆ ì ìš© ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ DB ìŠ¤í‚¤ë§ˆ ì ìš© ì‹¤íŒ¨: {e}")
            raise
        finally:
            conn.close()
    
    @contextmanager
    def get_connection(self):
        """DB ì»¤ë„¥ì…˜ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
        conn = sqlite3.connect(str(self.db_path))
        conn.row_factory = sqlite3.Row  # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜
        try:
            yield conn
        finally:
            conn.close()
    
    # ============================================
    # ìŠ¤ëƒ…ìƒ· ê´€ë¦¬
    # ============================================
    
    def save_snapshots(self, snapshots: List[Dict[str, Any]], snapshot_date: date = None):
        """
        ì¼ë³„ ì‹œì„¸ ìŠ¤ëƒ…ìƒ· ì €ì¥ (UPSERT)
        
        Args:
            snapshots: ì¢…ëª© ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            snapshot_date: ìŠ¤ëƒ…ìƒ· ë‚ ì§œ (Noneì´ë©´ ì˜¤ëŠ˜)
        """
        if not snapshots:
            logger.warning("âš ï¸ ì €ì¥í•  ìŠ¤ëƒ…ìƒ· ì—†ìŒ")
            return 0
        
        if snapshot_date is None:
            snapshot_date = date.today()
        
        saved_count = 0
        
        with self.get_connection() as conn:
            cursor = conn.cursor()
            
            for snap in snapshots:
                try:
                    cursor.execute("""
                        INSERT INTO stock_snapshots (
                            stock_code, snapshot_date, name, sector, sector_normalized,
                            open_price, high_price, low_price, close_price, volume,
                            market_cap, per, pbr, roe, debt_ratio,
                            dividend_yield, data_source
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                        ON CONFLICT(stock_code, snapshot_date) DO UPDATE SET
                            close_price = excluded.close_price,
                            per = excluded.per,
                            pbr = excluded.pbr,
                            roe = excluded.roe,
                            updated_at = CURRENT_TIMESTAMP
                    """, (
                        snap.get('code'),
                        snapshot_date,
                        snap.get('name'),
                        snap.get('sector'),
                        snap.get('sector_normalized'),
                        snap.get('open_price'),
                        snap.get('high_price'),
                        snap.get('low_price'),
                        snap.get('price') or snap.get('close_price'),
                        snap.get('volume'),
                        snap.get('market_cap'),
                        snap.get('per'),
                        snap.get('pbr'),
                        snap.get('roe'),
                        snap.get('debt_ratio'),
                        snap.get('dividend_yield'),
                        snap.get('data_source', 'KIS')
                    ))
                    saved_count += 1
                
                except Exception as e:
                    logger.error(f"âŒ ìŠ¤ëƒ…ìƒ· ì €ì¥ ì‹¤íŒ¨ ({snap.get('code')}): {e}")
                    continue
            
            conn.commit()
        
        logger.info(f"âœ… ìŠ¤ëƒ…ìƒ· ì €ì¥: {saved_count}/{len(snapshots)}ê°œ")
        return saved_count
    
    def get_latest_snapshots(self, max_age_days: int = 1) -> pd.DataFrame:
        """
        ìµœì‹  ìŠ¤ëƒ…ìƒ· ì¡°íšŒ
        
        Args:
            max_age_days: ìµœëŒ€ ê²½ê³¼ ì¼ìˆ˜
        
        Returns:
            DataFrame
        """
        cutoff_date = date.today() - timedelta(days=max_age_days)
        
        query = """
            SELECT * FROM stock_snapshots
            WHERE snapshot_date >= ?
            ORDER BY snapshot_date DESC, stock_code
        """
        
        with self.get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=(cutoff_date,))
        
        logger.debug(f"ğŸ“Š ìµœì‹  ìŠ¤ëƒ…ìƒ· ì¡°íšŒ: {len(df)}ê°œ (cutoff: {cutoff_date})")
        return df
    
    def get_snapshot_by_date(self, snapshot_date: date) -> pd.DataFrame:
        """
        íŠ¹ì • ë‚ ì§œì˜ ìŠ¤ëƒ…ìƒ· ì¡°íšŒ
        
        Args:
            snapshot_date: ë‚ ì§œ
        
        Returns:
            DataFrame
        """
        query = """
            SELECT * FROM stock_snapshots
            WHERE snapshot_date = ?
            ORDER BY stock_code
        """
        
        with self.get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=(snapshot_date,))
        
        logger.debug(f"ğŸ“Š {snapshot_date} ìŠ¤ëƒ…ìƒ· ì¡°íšŒ: {len(df)}ê°œ")
        return df
    
    def get_stock_history(self, stock_code: str, days: int = 90) -> pd.DataFrame:
        """
        ì¢…ëª©ë³„ ì‹œì„¸ ì´ë ¥ ì¡°íšŒ
        
        Args:
            stock_code: ì¢…ëª©ì½”ë“œ
            days: ì¡°íšŒ ì¼ìˆ˜
        
        Returns:
            DataFrame (ë‚ ì§œ ë‚´ë¦¼ì°¨ìˆœ)
        """
        query = """
            SELECT 
                snapshot_date as date,
                close_price as price,
                open_price, high_price, low_price, volume,
                per, pbr, roe, market_cap
            FROM stock_snapshots
            WHERE stock_code = ?
            ORDER BY snapshot_date DESC
            LIMIT ?
        """
        
        with self.get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=(stock_code, days))
        
        logger.debug(f"ğŸ“Š {stock_code} ì´ë ¥ ì¡°íšŒ: {len(df)}ì¼")
        return df
    
    def get_stale_stocks(self, max_age_days: int = 1) -> List[str]:
        """
        ì—…ë°ì´íŠ¸ í•„ìš”í•œ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ (ì¦ë¶„ ì—…ë°ì´íŠ¸ìš©)
        
        Args:
            max_age_days: ìµœëŒ€ ê²½ê³¼ ì¼ìˆ˜
        
        Returns:
            ì¢…ëª©ì½”ë“œ ë¦¬ìŠ¤íŠ¸
        """
        cutoff_date = date.today() - timedelta(days=max_age_days)
        
        # ìµœê·¼ ì—…ë°ì´íŠ¸ëœ ì¢…ëª©
        query = """
            SELECT DISTINCT stock_code
            FROM stock_snapshots
            WHERE snapshot_date >= ?
        """
        
        with self.get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=(cutoff_date,))
            recent_codes = set(df['stock_code'].tolist())
        
        # ì „ì²´ ì¢…ëª©ì€ stock_masterì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ì™¸ë¶€ API ì‚¬ìš©
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ìµœê·¼ 7ì¼ ë‚´ ë“±ì¥í•œ ì¢…ëª©ì„ ì „ì²´ë¡œ ê°€ì •
        query_all = """
            SELECT DISTINCT stock_code
            FROM stock_snapshots
            WHERE snapshot_date >= ?
        """
        
        with self.get_connection() as conn:
            df_all = pd.read_sql_query(query_all, conn, params=(date.today() - timedelta(days=7),))
            all_codes = set(df_all['stock_code'].tolist())
        
        stale_codes = all_codes - recent_codes
        
        logger.info(f"ğŸ“Š ì¦ë¶„ ì—…ë°ì´íŠ¸ ëŒ€ìƒ: {len(stale_codes)}ê°œ (ì „ì²´: {len(all_codes)})")
        return list(stale_codes)
    
    # ============================================
    # ì„¹í„° í†µê³„
    # ============================================
    
    def compute_sector_stats(self, snapshot_date: date = None) -> Dict[str, Dict]:
        """
        ì„¹í„° í†µê³„ ê³„ì‚° ë° ì €ì¥
        
        Args:
            snapshot_date: ê¸°ì¤€ ë‚ ì§œ (Noneì´ë©´ ì˜¤ëŠ˜)
        
        Returns:
            ì„¹í„°ë³„ í†µê³„ ë”•ì…”ë„ˆë¦¬
        """
        if snapshot_date is None:
            snapshot_date = date.today()
        
        logger.info(f"ğŸ“Š ì„¹í„° í†µê³„ ê³„ì‚° ì‹œì‘: {snapshot_date}")
        
        # í•´ë‹¹ ë‚ ì§œì˜ ìŠ¤ëƒ…ìƒ· ì¡°íšŒ
        df = self.get_snapshot_by_date(snapshot_date)
        
        if df.empty:
            logger.warning(f"âš ï¸ {snapshot_date} ìŠ¤ëƒ…ìƒ· ì—†ìŒ")
            return {}
        
        # ì„¹í„°ë³„ ê·¸ë£¹í™”
        sector_groups = df.groupby('sector_normalized')
        
        sector_stats = {}
        saved_count = 0
        
        with self.get_connection() as conn:
            cursor = conn.cursor()
            
            for sector, group in sector_groups:
                if sector is None or pd.isna(sector):
                    continue
                
                n = len(group)
                
                # ìµœì†Œ í‘œë³¸ í¬ê¸° í™•ì¸ (v2.3: 30 â†’ 5)
                # - n < 5: ê·¹ì†Œ í‘œë³¸, ì €ì¥ ì•ˆ í•¨
                # - 5 â‰¤ n < 10: ì €ì¥ â†’ ì‚¬ìš© ì‹œ ê¸€ë¡œë²Œë§Œ
                # - 10 â‰¤ n < 30: ì €ì¥ â†’ ì‚¬ìš© ì‹œ ê°€ì¤‘ í‰ê· 
                # - n â‰¥ 30: ì €ì¥ â†’ ì‚¬ìš© ì‹œ ì„¹í„° ìš°ì„ 
                if n < 5:
                    logger.debug(f"âš ï¸ {sector}: ê·¹ì†Œ í‘œë³¸ (n={n}) - ì €ì¥ ì•ˆ í•¨")
                    continue
                
                if n < 10:
                    logger.info(f"ğŸ“Œ {sector}: ì†Œí‘œë³¸ ì €ì¥ (n={n}) - ê¸€ë¡œë²Œ ëŒ€ì²´ ì˜ˆì •")
                elif n < 30:
                    logger.info(f"ğŸ“Œ {sector}: ì¤‘í‘œë³¸ ì €ì¥ (n={n}) - ê°€ì¤‘ í‰ê·  ì˜ˆì •")
                
                # PER, PBR, ROE ì¶”ì¶œ (ìœ íš¨ê°’ë§Œ)
                per_values = group['per'].dropna().values
                pbr_values = group['pbr'].dropna().values
                roe_values = group['roe'].dropna().values
                
                # í¼ì„¼íƒ€ì¼ ê³„ì‚°
                def calc_percentiles(values):
                    if len(values) == 0:
                        return {}
                    return {
                        'p10': np.percentile(values, 10),
                        'p25': np.percentile(values, 25),
                        'p50': np.percentile(values, 50),
                        'p75': np.percentile(values, 75),
                        'p90': np.percentile(values, 90),
                        'mean': np.mean(values),
                        'std': np.std(values),
                        'min': np.min(values),
                        'max': np.max(values),
                    }
                
                per_pct = calc_percentiles(per_values)
                pbr_pct = calc_percentiles(pbr_values)
                roe_pct = calc_percentiles(roe_values)
                
                # DB ì €ì¥
                try:
                    cursor.execute("""
                        INSERT INTO sector_stats (
                            sector, snapshot_date, sample_size,
                            per_p10, per_p25, per_p50, per_p75, per_p90, per_mean, per_std, per_min, per_max,
                            pbr_p10, pbr_p25, pbr_p50, pbr_p75, pbr_p90, pbr_mean, pbr_std, pbr_min, pbr_max,
                            roe_p10, roe_p25, roe_p50, roe_p75, roe_p90, roe_mean, roe_std, roe_min, roe_max
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                        ON CONFLICT(sector, snapshot_date) DO UPDATE SET
                            sample_size = excluded.sample_size,
                            per_p50 = excluded.per_p50,
                            pbr_p50 = excluded.pbr_p50,
                            roe_p50 = excluded.roe_p50
                    """, (
                        sector, snapshot_date, n,
                        per_pct.get('p10'), per_pct.get('p25'), per_pct.get('p50'), per_pct.get('p75'), per_pct.get('p90'),
                        per_pct.get('mean'), per_pct.get('std'), per_pct.get('min'), per_pct.get('max'),
                        pbr_pct.get('p10'), pbr_pct.get('p25'), pbr_pct.get('p50'), pbr_pct.get('p75'), pbr_pct.get('p90'),
                        pbr_pct.get('mean'), pbr_pct.get('std'), pbr_pct.get('min'), pbr_pct.get('max'),
                        roe_pct.get('p10'), roe_pct.get('p25'), roe_pct.get('p50'), roe_pct.get('p75'), roe_pct.get('p90'),
                        roe_pct.get('mean'), roe_pct.get('std'), roe_pct.get('min'), roe_pct.get('max')
                    ))
                    
                    saved_count += 1
                    
                    # ë°˜í™˜ìš© ë”•ì…”ë„ˆë¦¬ (ê¸°ì¡´ ìºì‹œ í˜•ì‹ í˜¸í™˜)
                    sector_stats[sector] = {
                        'sample_size': n,
                        'per_percentiles': per_pct,
                        'pbr_percentiles': pbr_pct,
                        'roe_percentiles': roe_pct,
                        'timestamp': datetime.now().isoformat()
                    }
                
                except Exception as e:
                    logger.error(f"âŒ ì„¹í„° í†µê³„ ì €ì¥ ì‹¤íŒ¨ ({sector}): {e}")
                    continue
            
            conn.commit()
        
        logger.info(f"âœ… ì„¹í„° í†µê³„ ê³„ì‚° ì™„ë£Œ: {saved_count}ê°œ ì„¹í„°")
        return sector_stats
    
    def get_sector_stats(self, snapshot_date: date = None) -> Dict[str, Dict]:
        """
        ì„¹í„° í†µê³„ ì¡°íšŒ (í”„ë¦¬ì»´í“¨íŒ…ëœ ê²ƒ)
        
        Args:
            snapshot_date: ê¸°ì¤€ ë‚ ì§œ (Noneì´ë©´ ìµœì‹ )
        
        Returns:
            ì„¹í„°ë³„ í†µê³„ ë”•ì…”ë„ˆë¦¬ (ê¸°ì¡´ pickle ìºì‹œ í˜•ì‹ í˜¸í™˜)
        """
        if snapshot_date is None:
            # ìµœì‹  ë‚ ì§œ ì¡°íšŒ
            with self.get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT MAX(snapshot_date) FROM sector_stats")
                result = cursor.fetchone()
                if result and result[0]:
                    snapshot_date = result[0]
                else:
                    logger.warning("âš ï¸ ì„¹í„° í†µê³„ ì—†ìŒ")
                    return {}
        
        query = """
            SELECT * FROM sector_stats
            WHERE snapshot_date = ?
        """
        
        with self.get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=(snapshot_date,))
        
        if df.empty:
            logger.warning(f"âš ï¸ {snapshot_date} ì„¹í„° í†µê³„ ì—†ìŒ")
            return {}
        
        # ê¸°ì¡´ ìºì‹œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        sector_stats = {}
        for _, row in df.iterrows():
            sector = row['sector']
            sector_stats[sector] = {
                'sample_size': row['sample_size'],
                'per_percentiles': {
                    'p10': row['per_p10'], 'p25': row['per_p25'], 'p50': row['per_p50'],
                    'p75': row['per_p75'], 'p90': row['per_p90'],
                    'mean': row['per_mean'], 'std': row['per_std'],
                    'min': row['per_min'], 'max': row['per_max']
                },
                'pbr_percentiles': {
                    'p10': row['pbr_p10'], 'p25': row['pbr_p25'], 'p50': row['pbr_p50'],
                    'p75': row['pbr_p75'], 'p90': row['pbr_p90'],
                    'mean': row['pbr_mean'], 'std': row['pbr_std'],
                    'min': row['pbr_min'], 'max': row['pbr_max']
                },
                'roe_percentiles': {
                    'p10': row['roe_p10'], 'p25': row['roe_p25'], 'p50': row['roe_p50'],
                    'p75': row['roe_p75'], 'p90': row['roe_p90'],
                    'mean': row['roe_mean'], 'std': row['roe_std'],
                    'min': row['roe_min'], 'max': row['roe_max']
                },
                'timestamp': str(snapshot_date) if snapshot_date else datetime.now().isoformat()
            }
        
        logger.info(f"âœ… ì„¹í„° í†µê³„ ì¡°íšŒ: {len(sector_stats)}ê°œ ({snapshot_date})")
        return sector_stats
    
    # ============================================
    # ìœ í‹¸ë¦¬í‹°
    # ============================================
    
    def get_stats(self) -> Dict[str, Any]:
        """DB í†µê³„ ì¡°íšŒ"""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            
            # ìŠ¤ëƒ…ìƒ· í†µê³„
            cursor.execute("SELECT COUNT(*) FROM stock_snapshots")
            total_snapshots = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(DISTINCT stock_code) FROM stock_snapshots")
            unique_stocks = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(DISTINCT snapshot_date) FROM stock_snapshots")
            unique_dates = cursor.fetchone()[0]
            
            cursor.execute("SELECT MIN(snapshot_date), MAX(snapshot_date) FROM stock_snapshots")
            date_range = cursor.fetchone()
            
            # ì„¹í„° í†µê³„
            cursor.execute("SELECT COUNT(DISTINCT sector) FROM sector_stats")
            unique_sectors = cursor.fetchone()[0]
            
            # ìµœì‹  ë‚ ì§œ
            cursor.execute("SELECT MAX(snapshot_date) FROM stock_snapshots")
            latest_date = cursor.fetchone()[0]
        
        return {
            'total_snapshots': total_snapshots,
            'unique_stocks': unique_stocks,
            'unique_dates': unique_dates,
            'date_range': date_range,
            'unique_sectors': unique_sectors,
            'latest_date': latest_date,
            'db_size_mb': self.db_path.stat().st_size / 1024 / 1024 if self.db_path.exists() else 0
        }
    
    def cleanup_old_data(self, keep_days: int = 365):
        """
        ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬
        
        Args:
            keep_days: ë³´ê´€ ì¼ìˆ˜ (ê¸°ë³¸ 1ë…„)
        """
        cutoff_date = date.today() - timedelta(days=keep_days)
        
        with self.get_connection() as conn:
            cursor = conn.cursor()
            
            # ìŠ¤ëƒ…ìƒ· ì‚­ì œ
            cursor.execute("DELETE FROM stock_snapshots WHERE snapshot_date < ?", (cutoff_date,))
            deleted_snapshots = cursor.rowcount
            
            # ì„¹í„° í†µê³„ ì‚­ì œ
            cursor.execute("DELETE FROM sector_stats WHERE snapshot_date < ?", (cutoff_date,))
            deleted_stats = cursor.rowcount
            
            conn.commit()
        
        logger.info(f"âœ… ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬: ìŠ¤ëƒ…ìƒ· {deleted_snapshots}ê°œ, í†µê³„ {deleted_stats}ê°œ")
        return deleted_snapshots, deleted_stats


# ============================================
# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤)
# ============================================
_global_db_cache: Optional[DBCacheManager] = None

def get_db_cache() -> DBCacheManager:
    """ì „ì—­ DB ìºì‹œ ë°˜í™˜ (ì‹±ê¸€í†¤)"""
    global _global_db_cache
    
    if _global_db_cache is None:
        _global_db_cache = DBCacheManager()
    
    return _global_db_cache


#!/usr/bin/env python3
"""
ë¦¬íŒ©í† ë§ëœ ë¶„ì„ ì‹œìŠ¤í…œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

ëª¨ë“ˆë³„ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê³  ë¹„êµí•©ë‹ˆë‹¤.
"""

import time
import psutil
import json
import statistics
from datetime import datetime
from typing import Dict, List, Any
import argparse

# ëª¨ë“ˆ import
try:
    from config_manager import ConfigManager
    from metrics import MetricsCollector
    from value_style_classifier import ValueStyleClassifier
    from uvs_eligibility_filter import UVSEligibilityFilter
    from financial_data_provider import FinancialDataProvider
    from enhanced_score_calculator import EnhancedScoreCalculator
    from analysis_models import AnalysisConfig, AnalysisResult, AnalysisStatus
except ImportError as e:
    print(f"âŒ ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
    print("ğŸ’¡ í•´ê²°: í•„ìš”í•œ ëª¨ë“ˆë“¤ì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    exit(1)


class PerformanceBenchmark:
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.results = {}
        self.memory_usage = []
        self.cpu_usage = []
        
    def measure_memory_cpu(self):
        """ë©”ëª¨ë¦¬ ë° CPU ì‚¬ìš©ëŸ‰ ì¸¡ì •"""
        process = psutil.Process()
        memory_info = process.memory_info()
        cpu_percent = process.cpu_percent()
        
        self.memory_usage.append(memory_info.rss / 1024 / 1024)  # MB
        self.cpu_usage.append(cpu_percent)
        
        return memory_info.rss / 1024 / 1024, cpu_percent
    
    def benchmark_config_manager(self, iterations: int = 1000) -> Dict[str, Any]:
        """ConfigManager ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        print("[TEST] ConfigManager ì„±ëŠ¥ ì¸¡ì • ì¤‘...")
        
        times = []
        memory_usage = []
        
        for i in range(iterations):
            start_time = time.time()
            start_memory, _ = self.measure_memory_cpu()
            
            # ConfigManager ì´ˆê¸°í™”
            config = ConfigManager()
            
            # ì„¤ì •ê°’ ì¡°íšŒ/ì„¤ì •
            config.set('test_key', f'test_value_{i}')
            value = config.get('test_key')
            
            end_time = time.time()
            end_memory, _ = self.measure_memory_cpu()
            
            times.append(end_time - start_time)
            memory_usage.append(end_memory - start_memory)
        
        return {
            'module': 'ConfigManager',
            'iterations': iterations,
            'avg_time': statistics.mean(times),
            'min_time': min(times),
            'max_time': max(times),
            'std_time': statistics.stdev(times) if len(times) > 1 else 0,
            'avg_memory': statistics.mean(memory_usage),
            'max_memory': max(memory_usage)
        }
    
    def benchmark_metrics_collector(self, iterations: int = 1000) -> Dict[str, Any]:
        """MetricsCollector ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        print("[TEST] MetricsCollector ì„±ëŠ¥ ì¸¡ì • ì¤‘...")
        
        times = []
        memory_usage = []
        
        for i in range(iterations):
            start_time = time.time()
            start_memory, _ = self.measure_memory_cpu()
            
            # MetricsCollector ì´ˆê¸°í™”
            metrics = MetricsCollector()
            
            # ë©”íŠ¸ë¦­ ê¸°ë¡
            for _ in range(i):
                metrics.record_api_call(success=True)
            for _ in range(i % 10):
                metrics.record_api_call(success=False)
            metrics.record_analysis_duration(i * 0.001)
            
            # í†µê³„ ì¡°íšŒ
            stats = metrics.get_summary()
            
            end_time = time.time()
            end_memory, _ = self.measure_memory_cpu()
            
            times.append(end_time - start_time)
            memory_usage.append(end_memory - start_memory)
        
        return {
            'module': 'MetricsCollector',
            'iterations': iterations,
            'avg_time': statistics.mean(times),
            'min_time': min(times),
            'max_time': max(times),
            'std_time': statistics.stdev(times) if len(times) > 1 else 0,
            'avg_memory': statistics.mean(memory_usage),
            'max_memory': max(memory_usage)
        }
    
    def benchmark_value_style_classifier(self, iterations: int = 1000) -> Dict[str, Any]:
        """ValueStyleClassifier ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        print("[TEST] ValueStyleClassifier ì„±ëŠ¥ ì¸¡ì • ì¤‘...")
        
        times = []
        memory_usage = []
        
        # í…ŒìŠ¤íŠ¸ìš© ë©”íŠ¸ë¦­ ë°ì´í„°
        test_metrics = {
            'valuation_pct': 20,
            'mos': 0.25,
            'price_pos': 0.3,
            'ev_ebit': 8,
            'interest_cov': 6.0,
            'accruals_risk': 0.1,
            'earnings_vol_sigma': 0.18,
            'sector_sigma_median': 0.25,
            'roic_z': 1.2,
            'f_score': 7,
            'eps_cagr': 0.15,
            'rev_cagr': 0.10
        }
        
        for i in range(iterations):
            start_time = time.time()
            start_memory, _ = self.measure_memory_cpu()
            
            # ValueStyleClassifier ì´ˆê¸°í™”
            classifier = ValueStyleClassifier()
            
            # ìŠ¤íƒ€ì¼ ë¶„ë¥˜
            result = classifier.classify_value_style(test_metrics)
            score = classifier.calculate_style_score(test_metrics)
            
            end_time = time.time()
            end_memory, _ = self.measure_memory_cpu()
            
            times.append(end_time - start_time)
            memory_usage.append(end_memory - start_memory)
        
        return {
            'module': 'ValueStyleClassifier',
            'iterations': iterations,
            'avg_time': statistics.mean(times),
            'min_time': min(times),
            'max_time': max(times),
            'std_time': statistics.stdev(times) if len(times) > 1 else 0,
            'avg_memory': statistics.mean(memory_usage),
            'max_memory': max(memory_usage)
        }
    
    def benchmark_uvs_eligibility_filter(self, iterations: int = 1000) -> Dict[str, Any]:
        """UVSEligibilityFilter ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        print("[TEST] UVSEligibilityFilter ì„±ëŠ¥ ì¸¡ì • ì¤‘...")
        
        times = []
        memory_usage = []
        
        # í…ŒìŠ¤íŠ¸ìš© ë©”íŠ¸ë¦­ ë°ì´í„°
        test_metrics = {
            'value_style_score': 85.0,
            'valuation_percentile': 25.0,
            'margin_of_safety': 0.20,
            'quality_score': 75.0,
            'risk_score': 0.25,
            'confidence_score': 0.8
        }
        
        for i in range(iterations):
            start_time = time.time()
            start_memory, _ = self.measure_memory_cpu()
            
            # UVSEligibilityFilter ì´ˆê¸°í™”
            filter = UVSEligibilityFilter()
            
            # ìê²© ê²€ì¦
            result = filter.check_uvs_eligibility(test_metrics)
            grade = filter.get_uvs_grade(result['uvs_score'])
            
            end_time = time.time()
            end_memory, _ = self.measure_memory_cpu()
            
            times.append(end_time - start_time)
            memory_usage.append(end_memory - start_memory)
        
        return {
            'module': 'UVSEligibilityFilter',
            'iterations': iterations,
            'avg_time': statistics.mean(times),
            'min_time': min(times),
            'max_time': max(times),
            'std_time': statistics.stdev(times) if len(times) > 1 else 0,
            'avg_memory': statistics.mean(memory_usage),
            'max_memory': max(memory_usage)
        }
    
    def benchmark_enhanced_score_calculator(self, iterations: int = 1000) -> Dict[str, Any]:
        """EnhancedScoreCalculator ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        print("[TEST] EnhancedScoreCalculator ì„±ëŠ¥ ì¸¡ì • ì¤‘...")
        
        times = []
        memory_usage = []
        
        # í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°
        test_data = {
            'pe_ratio': 15.0,
            'pb_ratio': 1.5,
            'roe': 15.0,
            'roa': 8.0,
            'debt_ratio': 30.0,
            'current_ratio': 2.0,
            'eps_growth': 0.12,
            'revenue_growth': 0.08,
            'beta': 1.2,
            'volatility': 0.25
        }
        
        # ë¶„ì„ êµ¬ì„± ì„¤ì •
        config = AnalysisConfig.get_default_config()
        
        for i in range(iterations):
            start_time = time.time()
            start_memory, _ = self.measure_memory_cpu()
            
            # EnhancedScoreCalculator ì´ˆê¸°í™”
            calculator = EnhancedScoreCalculator(config)
            
            # ì ìˆ˜ ê³„ì‚°
            score = calculator.calculate_score(test_data)
            breakdown = calculator.get_score_breakdown(test_data)
            
            end_time = time.time()
            end_memory, _ = self.measure_memory_cpu()
            
            times.append(end_time - start_time)
            memory_usage.append(end_memory - start_memory)
        
        return {
            'module': 'EnhancedScoreCalculator',
            'iterations': iterations,
            'avg_time': statistics.mean(times),
            'min_time': min(times),
            'max_time': max(times),
            'std_time': statistics.stdev(times) if len(times) > 1 else 0,
            'avg_memory': statistics.mean(memory_usage),
            'max_memory': max(memory_usage)
        }
    
    def benchmark_analysis_models(self, iterations: int = 1000) -> Dict[str, Any]:
        """AnalysisModels ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        print("[TEST] AnalysisModels ì„±ëŠ¥ ì¸¡ì • ì¤‘...")
        
        times = []
        memory_usage = []
        
        for i in range(iterations):
            start_time = time.time()
            start_memory, _ = self.measure_memory_cpu()
            
            # AnalysisResult ìƒì„±
            result = AnalysisResult(
                symbol=f'TEST{i:03d}',
                analysis_id=f'analysis_{i}',
                status=AnalysisStatus.COMPLETED,
                timestamp=datetime.now().isoformat(),
                financial_data={'revenue': 1000000000, 'net_income': 100000000},
                price_data={'current_price': 1500, 'market_cap': 15000000000},
                sector_data={'sector': 'Technology', 'industry': 'Software'},
                metrics={'pe_ratio': 15.0, 'pb_ratio': 1.5},
                scores={'total_score': 75.0, 'value_score': 80.0},
                recommendation='BUY',
                confidence_score=0.8,
                risk_level='LOW',
                analysis_duration=5.0,
                data_quality_score=85.0,
                error_messages=[]
            )
            
            # ë©”ì„œë“œ í˜¸ì¶œ
            is_successful = result.is_successful()
            has_errors = result.has_errors()
            summary = result.get_summary()
            result_dict = result.to_dict()
            
            end_time = time.time()
            end_memory, _ = self.measure_memory_cpu()
            
            times.append(end_time - start_time)
            memory_usage.append(end_memory - start_memory)
        
        return {
            'module': 'AnalysisModels',
            'iterations': iterations,
            'avg_time': statistics.mean(times),
            'min_time': min(times),
            'max_time': max(times),
            'std_time': statistics.stdev(times) if len(times) > 1 else 0,
            'avg_memory': statistics.mean(memory_usage),
            'max_memory': max(memory_usage)
        }
    
    def run_all_benchmarks(self, iterations: int = 1000) -> Dict[str, Any]:
        """ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        print(f"[BENCHMARK] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘ (ë°˜ë³µ íšŸìˆ˜: {iterations})")
        print("="*60)
        
        benchmarks = [
            self.benchmark_config_manager,
            self.benchmark_metrics_collector,
            self.benchmark_value_style_classifier,
            self.benchmark_uvs_eligibility_filter,
            self.benchmark_enhanced_score_calculator,
            self.benchmark_analysis_models
        ]
        
        results = []
        
        for benchmark_func in benchmarks:
            try:
                result = benchmark_func(iterations)
                results.append(result)
                print(f"[PASS] {result['module']} ì™„ë£Œ")
            except Exception as e:
                print(f"[FAIL] {benchmark_func.__name__} ì‹¤íŒ¨: {e}")
        
        # ì „ì²´ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬/CPU ì‚¬ìš©ëŸ‰
        process = psutil.Process()
        system_memory = process.memory_info().rss / 1024 / 1024  # MB
        system_cpu = process.cpu_percent()
        
        return {
            'timestamp': datetime.now().isoformat(),
            'iterations': iterations,
            'system_memory_mb': system_memory,
            'system_cpu_percent': system_cpu,
            'benchmarks': results
        }
    
    def generate_report(self, results: Dict[str, Any], output_file: str = "benchmark_report.json"):
        """ë²¤ì¹˜ë§ˆí¬ ë¦¬í¬íŠ¸ ìƒì„±"""
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"[REPORT] ë²¤ì¹˜ë§ˆí¬ ë¦¬í¬íŠ¸ ìƒì„±: {output_file}")
    
    def print_summary(self, results: Dict[str, Any]):
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*60)
        print("[SUMMARY] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½")
        print("="*60)
        
        print(f"[TIME] ì‹¤í–‰ ì‹œê°„: {results['timestamp']}")
        print(f"[ITER] ë°˜ë³µ íšŸìˆ˜: {results['iterations']}")
        print(f"[MEM] ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬: {results['system_memory_mb']:.1f} MB")
        print(f"[CPU] ì‹œìŠ¤í…œ CPU: {results['system_cpu_percent']:.1f}%")
        
        print("\n[PERF] ëª¨ë“ˆë³„ ì„±ëŠ¥:")
        print("-" * 60)
        print(f"{'ëª¨ë“ˆ':<25} {'í‰ê·  ì‹œê°„(ms)':<15} {'ìµœëŒ€ ë©”ëª¨ë¦¬(MB)':<15}")
        print("-" * 60)
        
        for benchmark in results['benchmarks']:
            avg_time_ms = benchmark['avg_time'] * 1000
            max_memory_mb = benchmark['max_memory']
            print(f"{benchmark['module']:<25} {avg_time_ms:<15.2f} {max_memory_mb:<15.2f}")
        
        print("-" * 60)
        
        # ê°€ì¥ ë¹ ë¥¸/ëŠë¦° ëª¨ë“ˆ ì°¾ê¸°
        if results['benchmarks']:
            fastest = min(results['benchmarks'], key=lambda x: x['avg_time'])
            slowest = max(results['benchmarks'], key=lambda x: x['avg_time'])
            
            print(f"\nğŸƒ ê°€ì¥ ë¹ ë¥¸ ëª¨ë“ˆ: {fastest['module']} ({fastest['avg_time']*1000:.2f}ms)")
            print(f"ğŸŒ ê°€ì¥ ëŠë¦° ëª¨ë“ˆ: {slowest['module']} ({slowest['avg_time']*1000:.2f}ms)")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ë¦¬íŒ©í† ë§ëœ ë¶„ì„ ì‹œìŠ¤í…œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
    parser.add_argument("--iterations", "-i", type=int, default=1000, help="ë°˜ë³µ íšŸìˆ˜")
    parser.add_argument("--output", "-o", default="benchmark_report.json", help="ì¶œë ¥ íŒŒì¼")
    parser.add_argument("--summary", "-s", action="store_true", help="ìš”ì•½ë§Œ ì¶œë ¥")
    
    args = parser.parse_args()
    
    try:
        benchmark = PerformanceBenchmark()
        results = benchmark.run_all_benchmarks(args.iterations)
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        benchmark.generate_report(results, args.output)
        
        # ìš”ì•½ ì¶œë ¥
        if args.summary:
            benchmark.print_summary(results)
        else:
            benchmark.print_summary(results)
            print(f"\n[REPORT] ìƒì„¸ ê²°ê³¼ëŠ” {args.output} íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    
    except KeyboardInterrupt:
        print("\n[STOP] ë²¤ì¹˜ë§ˆí¬ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\n[ERROR] ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


if __name__ == "__main__":
    main()

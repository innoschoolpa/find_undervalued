#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ëŒ€ëŸ‰ ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰ê¸°
- 1ë…„ ì´ìƒì˜ ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘
- ìƒìœ„ KOSPI ì¢…ëª© ëŒ€ìƒ
- ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ë° ì €ì¥
"""

import os
import time
import pandas as pd
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Any
import logging
from pathlib import Path
import json

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def collect_bulk_historical_data():
    """ëŒ€ëŸ‰ ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰"""
    try:
        from kis_data_provider import KISDataProvider
        from historical_data_cache import HistoricalDataCache
        from bulk_data_collector import BulkDataCollector
        
        # ì´ˆê¸°í™”
        kis_provider = KISDataProvider()
        cache_system = HistoricalDataCache()
        collector = BulkDataCollector(kis_provider, cache_system, max_workers=3)
        
        logger.info("ğŸš€ ëŒ€ëŸ‰ ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
        
        # 1. ìƒìœ„ KOSPI ì¢…ëª© ë°ì´í„° ìˆ˜ì§‘ (1ë…„)
        logger.info("ğŸ“Š ìƒìœ„ 50ê°œ KOSPI ì¢…ëª© ë°ì´í„° ìˆ˜ì§‘ (365ì¼)")
        results = collector.collect_top_kospi_stocks(count=50, days=365)
        
        if results.get('success', True):  # successê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ì ìœ¼ë¡œ ì„±ê³µìœ¼ë¡œ ê°„ì£¼
            summary = results.get('summary', {})
            logger.info(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {summary.get('successful', 0)}ê°œ ì„±ê³µ, {summary.get('failed', 0)}ê°œ ì‹¤íŒ¨")
            logger.info(f"ğŸ“ˆ ì´ ë ˆì½”ë“œ: {summary.get('total_records', 0)}ê±´")
            logger.info(f"â±ï¸ ì†Œìš” ì‹œê°„: {summary.get('duration_seconds', 0):.1f}ì´ˆ")
            
            # 2. ì‹¤íŒ¨í•œ ì¢…ëª© ì¬ìˆ˜ì§‘
            failed_symbols = summary.get('failed_symbols', [])
            if failed_symbols:
                logger.info(f"ğŸ”„ ì‹¤íŒ¨í•œ ì¢…ëª© ì¬ìˆ˜ì§‘: {len(failed_symbols)}ê°œ")
                retry_results = collector.retry_failed_collections(failed_symbols, days=365)
                
                retry_summary = retry_results.get('summary', {})
                logger.info(f"âœ… ì¬ìˆ˜ì§‘ ì™„ë£Œ: {retry_summary.get('successful', 0)}ê°œ ì„±ê³µ")
            
            # 3. ìˆ˜ì§‘ ë³´ê³ ì„œ ì €ì¥
            report_file = collector.save_collection_report(results)
            if report_file:
                logger.info(f"ğŸ“„ ìˆ˜ì§‘ ë³´ê³ ì„œ ì €ì¥: {report_file}")
            
            # 4. ìºì‹œ ìƒíƒœ í™•ì¸
            cache_status = cache_system.get_cache_status()
            logger.info(f"ğŸ’¾ ìºì‹œ ìƒíƒœ: {cache_status.get('total_symbols', 0)}ê°œ ì¢…ëª©, {cache_status.get('total_records', 0)}ê±´")
            
            return True
        else:
            logger.error(f"âŒ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {results.get('error', 'Unknown error')}")
            return False
            
    except Exception as e:
        logger.error(f"âŒ ëŒ€ëŸ‰ ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        return False

def collect_specific_symbols():
    """íŠ¹ì • ì¢…ëª© ë°ì´í„° ìˆ˜ì§‘"""
    try:
        from kis_data_provider import KISDataProvider
        from historical_data_cache import HistoricalDataCache
        from bulk_data_collector import BulkDataCollector
        
        # ì´ˆê¸°í™”
        kis_provider = KISDataProvider()
        cache_system = HistoricalDataCache()
        collector = BulkDataCollector(kis_provider, cache_system, max_workers=2)
        
        # ì£¼ìš” ì¢…ëª© ë¦¬ìŠ¤íŠ¸
        major_symbols = [
            '005930',  # ì‚¼ì„±ì „ì
            '000660',  # SKí•˜ì´ë‹‰ìŠ¤
            '373220',  # LGì—ë„ˆì§€ì†”ë£¨ì…˜
            '207940',  # ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤
            '012450',  # í•œí™”ì—ì–´ë¡œìŠ¤í˜ì´ìŠ¤
            '006400',  # ì‚¼ì„±SDI
            '035420',  # NAVER
            '051910',  # LGí™”í•™
            '068270',  # ì…€íŠ¸ë¦¬ì˜¨
            '323410',  # ì¹´ì¹´ì˜¤ë±…í¬
        ]
        
        logger.info(f"ğŸ¯ ì£¼ìš” ì¢…ëª© ë°ì´í„° ìˆ˜ì§‘: {len(major_symbols)}ê°œ")
        
        # 2ë…„ ë°ì´í„° ìˆ˜ì§‘
        results = collector.collect_historical_data_bulk(major_symbols, days=730)
        
        summary = results.get('summary', {})
        logger.info(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {summary.get('successful', 0)}ê°œ ì„±ê³µ")
        logger.info(f"ğŸ“ˆ ì´ ë ˆì½”ë“œ: {summary.get('total_records', 0)}ê±´")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ íŠ¹ì • ì¢…ëª© ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        return False

def validate_data_quality():
    """ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
    try:
        from historical_data_cache import HistoricalDataCache
        
        cache_system = HistoricalDataCache()
        status = cache_system.get_cache_status()
        
        logger.info("ğŸ” ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì‹œì‘")
        
        # ê° ì¢…ëª©ë³„ ë°ì´í„° í’ˆì§ˆ í™•ì¸
        recent_updates = status.get('recent_updates', [])
        
        for update in recent_updates[:10]:  # ìƒìœ„ 10ê°œë§Œ í™•ì¸
            symbol = update['symbol']
            record_count = update['record_count']
            
            # ë°ì´í„° ì¡°íšŒ
            data = cache_system.get_cached_data(symbol, '2024-01-01', '2024-12-31')
            
            if data is not None and not data.empty:
                # ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
                quality_score = calculate_data_quality_score(data)
                logger.info(f"ğŸ“Š {symbol}: {len(data)}ê±´, í’ˆì§ˆ {quality_score:.2f}")
            else:
                logger.warning(f"âš ï¸ {symbol}: ë°ì´í„° ì—†ìŒ")
        
        logger.info("âœ… ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì˜¤ë¥˜: {e}")
        return False

def calculate_data_quality_score(data: pd.DataFrame) -> float:
    """ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
    try:
        if data.empty:
            return 0.0
        
        score = 1.0
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_columns = ['date', 'close']
        missing_columns = [col for col in required_columns if col not in data.columns]
        if missing_columns:
            score -= 0.3
        
        # ëˆ„ë½ ë°ì´í„° ì²´í¬
        missing_ratio = data.isnull().sum().sum() / (len(data) * len(data.columns))
        score -= missing_ratio * 0.3
        
        # ê°€ê²© ë°ì´í„° ìœ íš¨ì„±
        if 'close' in data.columns:
            negative_prices = (data['close'] <= 0).sum()
            score -= (negative_prices / len(data)) * 0.4
        
        # ì‹œê°„ ì—°ì†ì„±
        if len(data) > 1:
            data_sorted = data.sort_values('date')
            date_diff = data_sorted['date'].diff().dt.days
            gaps = (date_diff > 7).sum()  # 7ì¼ ì´ìƒ ê°„ê²©
            score -= (gaps / len(data)) * 0.2
        
        return max(0.0, min(1.0, score))
        
    except Exception as e:
        logger.error(f"ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
        return 0.0

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸš€ ëŒ€ëŸ‰ ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ ì‹œì‘")
    
    # 1. ì£¼ìš” ì¢…ëª© ë°ì´í„° ìˆ˜ì§‘
    logger.info("\n=== 1ë‹¨ê³„: ì£¼ìš” ì¢…ëª© ë°ì´í„° ìˆ˜ì§‘ ===")
    if collect_specific_symbols():
        logger.info("âœ… ì£¼ìš” ì¢…ëª© ìˆ˜ì§‘ ì™„ë£Œ")
    else:
        logger.error("âŒ ì£¼ìš” ì¢…ëª© ìˆ˜ì§‘ ì‹¤íŒ¨")
        return
    
    # 2. ëŒ€ëŸ‰ ë°ì´í„° ìˆ˜ì§‘
    logger.info("\n=== 2ë‹¨ê³„: ëŒ€ëŸ‰ ë°ì´í„° ìˆ˜ì§‘ ===")
    if collect_bulk_historical_data():
        logger.info("âœ… ëŒ€ëŸ‰ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
    else:
        logger.error("âŒ ëŒ€ëŸ‰ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
        return
    
    # 3. ë°ì´í„° í’ˆì§ˆ ê²€ì¦
    logger.info("\n=== 3ë‹¨ê³„: ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ===")
    if validate_data_quality():
        logger.info("âœ… ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ")
    else:
        logger.error("âŒ ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨")
        return
    
    logger.info("\nğŸ‰ ëª¨ë“  ë°ì´í„° ìˆ˜ì§‘ ì‘ì—… ì™„ë£Œ!")

if __name__ == "__main__":
    main()


